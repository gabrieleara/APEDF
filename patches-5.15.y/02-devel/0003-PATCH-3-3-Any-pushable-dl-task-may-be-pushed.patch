From a9568e9a40e7ed841b2974eb5b29fa9c3bcb27f0 Mon Sep 17 00:00:00 2001
From: Gabriele Ara <gabriele.ara@santannapisa.it>
Date: Thu, 24 Nov 2022 12:26:00 +0100
Subject: [PATCH 3/3] Any pushable dl task may be pushed

---
 kernel/sched/deadline.c | 151 +++++++++++++++++++++++-----------------
 1 file changed, 87 insertions(+), 64 deletions(-)

diff --git a/kernel/sched/deadline.c b/kernel/sched/deadline.c
index 270cc0c54..47da96adb 100644
--- a/kernel/sched/deadline.c
+++ b/kernel/sched/deadline.c
@@ -2197,102 +2197,125 @@ static void pushable_dl_task_checkbugs(struct rq *rq, struct task_struct *p)
 	BUG_ON(!dl_task(p));
 }
 
-static struct task_struct *pick_next_pushable_dl_task(struct rq *rq)
-{
-	struct task_struct *p;
+static struct task_struct *pushable_dl_task(struct rq *rq, struct rb_node *node) {
+	struct task_struct *p = pushable_dl_task_quick(rq, node);
+	pushable_dl_task_checkbugs(rq, p);
+	return p;
+}
 
-	if (!has_pushable_dl_tasks(rq))
-		return NULL;
+static struct rb_node *pushable_dl_find(struct rq *rq, struct task_struct *task)
+{
+	struct rb_node *node;
+	struct task_struct *node_task;
+	for (node = pushable_dl_first(rq); node; node = rb_next(node)) {
+		node_task = pushable_dl_task_quick(rq, node);
+		if (node_task == task) {
+			pushable_dl_task_checkbugs(rq, task);
+			break;
+		}
+	}
+	return node;
+}
 
-	p = pushable_dl_task_quick(rq, pushable_dl_first(rq));
+struct rb_node *pushable_dl_next(struct rq *rq, struct rb_node *node,
+				 int *trycount)
+{
+	if (node) {
+		return rb_next(node);
+	}
 
-	pushable_dl_task_checkbugs(rq, p);
+	// Maximum number of re-tries from the beginning
+	if (*trycount >= 3) {
+		return NULL;
+	}
 
-	return p;
+	// Start again from the beginning
+	(*trycount)++;
+	return pushable_dl_first(rq);
 }
 
 /*
- * See if the non running -deadline tasks on this rq
- * can be sent to some other CPU where they can preempt
- * and start executing.
+ * If the current CPU is overcommitted (more bw than capacity), see if at least
+ * one non running -deadline tasks on this rq can be sent to some other CPU
+ * where they can fit.
+ *
+ * Returns 0 if no tasks have been pushed away, true if one task has been pushed
+ * successfully.
  */
 static int push_dl_task(struct rq *rq)
 {
 	struct task_struct *next_task;
 	struct rq *later_rq = NULL;
-	int ret = 0;
-
-	next_task = pick_next_pushable_dl_task(rq);
-	if (!next_task)
-		return 0;
+	struct rb_node *node;
+	int trycount = 0;
 
 	if (is_migration_disabled(next_task))
 		return 0;
 
-	if (WARN_ON(next_task == rq->curr))
-		return 0;
-
-	/* We might release rq lock */
-retry:
-	get_task_struct(next_task);
-	later_rq = find_lock_later_rq_ff(next_task, rq);
-	if (!later_rq) {
-		if (rq->dl.this_bw < rq->dl.max_bw) {
-			put_task_struct(next_task);
+	/*
+	 * This loop may start again from the beginning up to a fixed number of
+	 * tries by setting node=NULL before getting the next one
+	 */
+	for (node = pushable_dl_first(rq); node;
+	     node = pushable_dl_next(rq, node, &trycount)) {
+		next_task = pushable_dl_task(rq, node);
 
+		/* Do we still WANT to push tasks? */
+		if (rq->dl.this_bw <= rq->dl.max_bw) {
 			return 0;
 		}
 
+		get_task_struct(next_task);
+
 		/* Will lock the rq it'll find */
-		later_rq = find_lock_later_rq(next_task, rq);
-		if (!later_rq) {
-			struct task_struct *task;
+		later_rq = find_lock_later_rq_ff(next_task, rq);
+		/* Do we still WANT to push tasks? */
+		if (rq->dl.this_bw <= rq->dl.max_bw) {
+			put_task_struct(next_task);
+			return 0;
+		}
 
+		/*
+		 * Since we had to release the lock to find later_rq, check
+		 * whether the task is still there. Note that this may return a
+		 * different node than the original one (we are fine with it) or
+		 * NULL if we have to start again from the beginning.
+		 */
+		node = pushable_dl_find(rq, next_task);
+		if (node == NULL || later_rq == NULL) {
 			/*
-			 * We must check all this again, since
-			 * find_lock_later_rq releases rq->lock and it is
-			 * then possible that next_task has migrated.
+			 * Task has already moved away or we cannot push it,
+			 * either way we want to try find a new task to push.
 			 */
-			task = pick_next_pushable_dl_task(rq);
-			if (task == next_task) {
-				/*
-				 * The task is still there. We don't try
-				 * again, some other CPU will pull it when ready.
-				 */
-				goto out;
-			}
-
-			if (!task)
-				/* No more tasks */
-				goto out;
-
 			put_task_struct(next_task);
-			next_task = task;
-			goto retry;
+			continue;
 		}
-	}
-
-	deactivate_task(rq, next_task, 0);
-	set_task_cpu(next_task, later_rq->cpu);
-
-	/*
-	 * Update the later_rq clock here, because the clock is used
-	 * by the cpufreq_update_util() inside __add_running_bw().
-	 */
-	update_rq_clock(later_rq);
-	activate_task(later_rq, next_task, ENQUEUE_NOCLOCK);
-	ret = 1;
 
-	resched_curr(later_rq);
+		/*
+		 * Task is still in the list of pushable ones and it can be
+		 * moved to later_rq, so we do it now.
+		 */
+		deactivate_task(rq, next_task, 0);
+		set_task_cpu(next_task, later_rq->cpu);
 
-	double_unlock_balance(rq, later_rq);
+		/*
+		 * Update the later_rq clock here, because the clock is used
+		 * by the cpufreq_update_util() inside __add_running_bw().
+		 */
+		update_rq_clock(later_rq);
+		activate_task(later_rq, next_task, ENQUEUE_NOCLOCK);
+		resched_curr(later_rq);
 
-out:
-	put_task_struct(next_task);
+		double_unlock_balance(rq, later_rq);
+		put_task_struct(next_task);
+		return 1;
+	}
 
-	return ret;
+	/* We couldn't find tasks to push */
+	return 0;
 }
 
+
 static void push_dl_tasks(struct rq *rq)
 {
 	/* push_dl_task() will return true if it moved a -deadline task */
-- 
2.25.1

