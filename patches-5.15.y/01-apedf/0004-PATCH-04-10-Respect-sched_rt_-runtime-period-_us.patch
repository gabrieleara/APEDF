From 71db7b771b50d8b380a71f3f0339b44ed820b9d8 Mon Sep 17 00:00:00 2001
From: luca abeni <luca.abeni@santannapisa.it>
Date: Thu, 18 Nov 2021 13:13:35 +0000
Subject: [PATCH 04/10] Respect sched_rt_{runtime,period}_us

---
 kernel/sched/deadline.c | 17 ++++++++++-------
 kernel/sched/sched.h    |  2 ++
 2 files changed, 12 insertions(+), 7 deletions(-)

diff --git a/kernel/sched/deadline.c b/kernel/sched/deadline.c
index d9686f8ec..02f51a565 100644
--- a/kernel/sched/deadline.c
+++ b/kernel/sched/deadline.c
@@ -1121,7 +1121,7 @@ static enum hrtimer_restart dl_task_timer(struct hrtimer *timer)
 	 * Queueing this task back might have overloaded rq, check if we need
 	 * to kick someone away.
 	 */
-	if (has_pushable_dl_tasks(rq)) {
+	if (rq->dl.this_bw > rq->dl.max_bw && has_pushable_dl_tasks(rq)) {
 		/*
 		 * Nothing relies on rq->lock after this, so its safe to drop
 		 * rq->lock.
@@ -1695,7 +1695,7 @@ select_task_rq_dl(struct task_struct *p, int cpu, int flags)
 	target = find_later_rq_ff(p);
 	if (target >= 0) {
 		cpu = target;
-	} else if (rq->dl.this_bw > 1 << BW_SHIFT) {
+	} else if (rq->dl.this_bw > rq->dl.max_bw) {
 		/* FF did not work: Try gEDF */
 		target = find_later_rq(p);
 		if (target >= 0 &&
@@ -1832,7 +1832,7 @@ static void set_next_task_dl(struct rq *rq, struct task_struct *p, bool first)
 	if (rq->curr->sched_class != &dl_sched_class)
 		update_dl_rq_load_avg(rq_clock_pelt(rq), rq, 0);
 
-	if (rq->dl.this_bw > 1 << BW_SHIFT) deadline_queue_push_tasks(rq);
+	if (rq->dl.this_bw > rq->dl.max_bw) deadline_queue_push_tasks(rq);
 }
 
 static struct sched_dl_entity *pick_next_dl_entity(struct rq *rq,
@@ -2053,7 +2053,7 @@ static int find_later_rq_ff(struct task_struct *task)
 		struct rq *rq = cpu_rq(i);
 		u64 added_bw = i == task_cpu(task) ? 0 : task->dl.dl_bw;
 
-		if (cpumask_test_cpu(i, task->cpus_ptr) && (rq->dl.this_bw + added_bw < 1 << BW_SHIFT)) {
+		if (cpumask_test_cpu(i, task->cpus_ptr) && (rq->dl.this_bw + added_bw < rq->dl.max_bw)) {
 			return i;
 		}
 	}
@@ -2151,7 +2151,7 @@ static struct rq *find_lock_later_rq_ff(struct task_struct *task, struct rq *rq)
 		 * its earliest one has a later deadline than our
 		 * task, the rq is a good one.
 		 */
-		if (rq->dl.this_bw + task->dl.dl_bw < 1 << BW_SHIFT)
+		if (rq->dl.this_bw + task->dl.dl_bw < rq->dl.max_bw)
 			break;
 
 		/* Otherwise we try again. */
@@ -2210,7 +2210,7 @@ static int push_dl_task(struct rq *rq)
 	later_rq = find_lock_later_rq_ff(next_task, rq);
 	if (!later_rq) {
 		struct task_struct *task;
-		if (rq->dl.this_bw < 1 << BW_SHIFT)
+		if (rq->dl.this_bw < rq->dl.max_bw)
 			return 0;
 
 retry:
@@ -2541,7 +2541,7 @@ static void switched_to_dl(struct rq *rq, struct task_struct *p)
 
 	if (rq->curr != p) {
 #ifdef CONFIG_SMP
-		if (rq->dl.this_bw > 1 << BW_SHIFT)
+		if (rq->dl.this_bw > rq->dl.max_bw)
 			deadline_queue_push_tasks(rq);
 #endif
 		if (dl_task(rq->curr))
@@ -2662,11 +2662,14 @@ static void init_dl_rq_bw_ratio(struct dl_rq *dl_rq)
 	if (global_rt_runtime() == RUNTIME_INF) {
 		dl_rq->bw_ratio = 1 << RATIO_SHIFT;
 		dl_rq->extra_bw = 1 << BW_SHIFT;
+		dl_rq->max_bw   = 1 << BW_SHIFT;
 	} else {
 		dl_rq->bw_ratio = to_ratio(global_rt_runtime(),
 			  global_rt_period()) >> (BW_SHIFT - RATIO_SHIFT);
 		dl_rq->extra_bw = to_ratio(global_rt_period(),
 						    global_rt_runtime());
+		dl_rq->max_bw   = to_ratio(global_rt_period(),
+						    global_rt_runtime());
 	}
 }
 
diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index e49902898..bea4f2115 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -729,6 +729,8 @@ struct dl_rq {
 	 * by the GRUB algorithm.
 	 */
 	u64			bw_ratio;
+
+	u64			max_bw;
 };
 
 #ifdef CONFIG_FAIR_GROUP_SCHED
-- 
2.25.1

