From d8259a4fdd0b9bb176d0a198d7aaa00dd671fd1e Mon Sep 17 00:00:00 2001
From: Gabriele Ara <gabriele.ara@santannapisa.it>
Date: Wed, 16 Nov 2022 13:12:28 +0100
Subject: Any pushable dl task may be pushed

---
 kernel/sched/deadline.c | 188 ++++++++++++++++++++++++----------------
 1 file changed, 114 insertions(+), 74 deletions(-)

diff --git a/kernel/sched/deadline.c b/kernel/sched/deadline.c
index 26c5c8200..b8320e9fd 100644
--- a/kernel/sched/deadline.c
+++ b/kernel/sched/deadline.c
@@ -1246,8 +1246,12 @@ static void update_curr_dl(struct rq *rq)
 			dl_se->dl_overrun = 1;
 
 		__dequeue_task_dl(rq, curr, 0);
-		if (unlikely(dl_se->dl_boosted || !start_dl_timer(curr)))
+		if (unlikely(dl_se->dl_boosted || !start_dl_timer(curr))) {
 			enqueue_task_dl(rq, curr, ENQUEUE_REPLENISH);
+			if (rq->dl.this_bw > rq->dl.max_bw) {
+				deadline_queue_push_tasks(rq);
+			}
+		}
 
 		if (!is_leftmost(curr, &rq->dl))
 			resched_curr(rq);
@@ -1801,9 +1805,14 @@ static void put_prev_task_dl(struct rq *rq, struct task_struct *p)
 	update_curr_dl(rq);
 
 	update_dl_rq_load_avg(rq_clock_pelt(rq), rq, 1);
-/* No! A preempted task cannot migrate!!!
-	if (on_dl_rq(&p->dl) && p->nr_cpus_allowed > 1)
-		enqueue_pushable_dl_task(rq, p);*/
+	// FIXME: No! A preempted task cannot migrate!!!
+	if (on_dl_rq(&p->dl) && p->nr_cpus_allowed > 1) {
+		enqueue_pushable_dl_task(rq, p);
+
+		if (rq->dl.this_bw > rq->dl.max_bw) {
+			deadline_queue_push_tasks(rq);
+		}
+	}
 }
 
 /*
@@ -1976,7 +1985,7 @@ static int find_later_rq_ff(struct task_struct *task)
 		struct rq *rq = cpu_rq(i);
 		u64 added_bw = i == task_cpu(task) ? 0 : task->dl.dl_bw;
 
-		if (cpumask_test_cpu(i, task->cpus_ptr) && (rq->dl.this_bw + added_bw < rq->dl.max_bw)) {
+		if (cpumask_test_cpu(i, task->cpus_ptr) && (rq->dl.this_bw + added_bw <= rq->dl.max_bw)) {
 			return i;
 		}
 	}
@@ -2069,12 +2078,9 @@ static struct rq *find_lock_later_rq_ff(struct task_struct *task, struct rq *rq)
 			}
 		}
 
-		/*
-		 * If the rq we found has no -deadline task, or
-		 * its earliest one has a later deadline than our
-		 * task, the rq is a good one.
-		 */
-		if (later_rq->dl.this_bw + task->dl.dl_bw < later_rq->dl.max_bw)
+		// If the task may still fit in the capacity of the target
+		// runqueue, we found a valid destionation.
+		if (later_rq->dl.this_bw + task->dl.dl_bw <= later_rq->dl.max_bw)
 			break;
 
 		/* Otherwise we try again. */
@@ -2085,103 +2091,137 @@ static struct rq *find_lock_later_rq_ff(struct task_struct *task, struct rq *rq)
 	return later_rq;
 }
 
-static struct task_struct *pick_next_pushable_dl_task(struct rq *rq)
+static struct rb_node *pushable_dl_first(struct rq *rq)
 {
-	struct task_struct *p;
-
 	if (!has_pushable_dl_tasks(rq))
 		return NULL;
+	return rq->dl.pushable_dl_tasks_root.rb_leftmost;
+}
 
-	p = rb_entry(rq->dl.pushable_dl_tasks_root.rb_leftmost,
-		     struct task_struct, pushable_dl_tasks);
+static struct task_struct *pushable_dl_task_quick(struct rq *rq,
+						  struct rb_node *node)
+{
+	return rb_entry(node, struct task_struct, pushable_dl_tasks);
+}
 
+static void pushable_dl_task_checkbugs(struct rq *rq, struct task_struct *p)
+{
 	BUG_ON(rq->cpu != task_cpu(p));
 	BUG_ON(task_current(rq, p));
 	BUG_ON(p->nr_cpus_allowed <= 1);
-
 	BUG_ON(!task_on_rq_queued(p));
 	BUG_ON(!dl_task(p));
+}
 
+static struct task_struct *pushable_dl_task(struct rq *rq, struct rb_node *node)
+{
+	struct task_struct *p = pushable_dl_task_quick(rq, node);
+	pushable_dl_task_checkbugs(rq, p);
 	return p;
 }
 
+static struct rb_node *pushable_dl_find(struct rq *rq, struct task_struct *task)
+{
+	struct rb_node *node;
+	struct task_struct *node_task;
+	for (node = pushable_dl_first(rq); node; node = rb_next(node)) {
+		node_task = pushable_dl_task_quick(rq, node);
+		if (node_task == task) {
+			pushable_dl_task_checkbugs(rq, task);
+			break;
+		}
+	}
+	return node;
+}
+
+struct rb_node *pushable_dl_next(struct rq *rq, struct rb_node *node,
+				 int *trycount)
+{
+	if (node) {
+		return rb_next(node);
+	}
+
+	// Maximum number of re-tries from the beginning
+	if (*trycount >= 3) {
+		return NULL;
+	}
+
+	// Start again from the beginning
+	(*trycount)++;
+	return pushable_dl_first(rq);
+}
+
 /*
- * See if the non running -deadline tasks on this rq
- * can be sent to some other CPU where they can preempt
- * and start executing.
+ * If the current CPU is overcommitted (more bw than capacity), see if at least
+ * one non running -deadline tasks on this rq can be sent to some other CPU
+ * where they can fit.
+ *
+ * Returns 0 if no tasks have been pushed away, true if one task has been pushed
+ * successfully.
  */
 static int push_dl_task(struct rq *rq)
 {
-	struct task_struct *next_task;
+	struct task_struct *task;
 	struct rq *later_rq = NULL;
-	int ret = 0;
 
-	next_task = pick_next_pushable_dl_task(rq);
-	if (!next_task)
-		return 0;
-
-	if (WARN_ON(next_task == rq->curr))
-		return 0;
-
-	/* We might release rq lock */
-retry:
-	get_task_struct(next_task);
-	later_rq = find_lock_later_rq_ff(next_task, rq);
-	if (!later_rq) {
-		if (rq->dl.this_bw < rq->dl.max_bw) {
-			put_task_struct(next_task);
+	// This loop may start again from the beginning up to a fixed number of
+	// tries by setting node=NULL before getting the next one
+	struct rb_node *node;
+	int trycount = 0;
+	for (node = pushable_dl_first(rq); node;
+	     node = pushable_dl_next(rq, node, &trycount)) {
+		task = pushable_dl_task(rq, node);
 
+		// Do we still WANT to push tasks?
+		if (rq->dl.this_bw <= rq->dl.max_bw) {
 			return 0;
 		}
 
-		/* Will lock the rq it'll find */
-		later_rq = find_lock_later_rq(next_task, rq);
-		if (!later_rq) {
-			struct task_struct *task;
+		get_task_struct(task);
 
-			/*
-			 * We must check all this again, since
-			 * find_lock_later_rq releases rq->lock and it is
-			 * then possible that next_task has migrated.
-			 */
-			task = pick_next_pushable_dl_task(rq);
-			if (task == next_task) {
-				/*
-				 * The task is still there. We don't try
-				 * again, some other CPU will pull it when ready.
-				 */
-				goto out;
-			}
+		// Will lock the rq it'll find
+		later_rq = find_lock_later_rq_ff(task, rq);
 
-			if (!task)
-				/* No more tasks */
-				goto out;
+		// Do we still WANT to push tasks?
+		if (rq->dl.this_bw <= rq->dl.max_bw) {
+			put_task_struct(task);
+			return 0;
+		}
 
-			put_task_struct(next_task);
-			next_task = task;
-			goto retry;
+		// Since we had to release the lock to find later_rq, check
+		// whether the task is still there. Note that this may return a
+		// different node than the original one (we are fine with it) or
+		// NULL if we have to start again from the beginning.
+		node = pushable_dl_find(rq, task);
+		if (node == NULL || later_rq == NULL) {
+			// Task has already moved away or we cannot push it,
+			// either way we want to try find a new task to push.
+			put_task_struct(task);
+			continue;
 		}
-	}
 
-	deactivate_task(rq, next_task, 0);
-	set_task_cpu(next_task, later_rq->cpu);
+		// Task is still in the list of pushable ones and it can be
+		// moved to later_rq, so we do it now.
 
-	/*
-	 * Update the later_rq clock here, because the clock is used
-	 * by the cpufreq_update_util() inside __add_running_bw().
-	 */
-	update_rq_clock(later_rq);
-	activate_task(later_rq, next_task, ENQUEUE_NOCLOCK);
-	ret = 1;
+		deactivate_task(rq, task, 0);
+		set_task_cpu(task, later_rq->cpu);
 
-	resched_curr(later_rq);
+		/*
+		 * Update the later_rq clock here, because the clock is used
+		 * by the cpufreq_update_util() inside __add_running_bw().
+		 */
+		update_rq_clock(later_rq);
+		activate_task(later_rq, task, ENQUEUE_NOCLOCK);
 
-	double_unlock_balance(rq, later_rq);
+		resched_curr(later_rq);
 
-out:
-	put_task_struct(next_task);
+		double_unlock_balance(rq, later_rq);
+		put_task_struct(task);
+		return 1;
+	}
 
-	return ret;
+	// We couldn't find tasks to push
+	return 0;
 }
 
 static void push_dl_tasks(struct rq *rq)
-- 
2.38.1

