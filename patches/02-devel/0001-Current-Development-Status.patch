From dbddabad66a8daa5279ce68ae5b8529f355e153a Mon Sep 17 00:00:00 2001
From: Gabriele Ara <gabriele.ara@santannapisa.it>
Date: Tue, 15 Nov 2022 11:43:26 +0100
Subject: Current Development Status

---
 Makefile                         |  2 +-
 kernel/sched/cpufreq_schedutil.c | 25 +++++++++++++++++++++
 kernel/sched/deadline.c          | 37 ++++++++++++++++++++++++++++----
 3 files changed, 59 insertions(+), 5 deletions(-)

diff --git a/Makefile b/Makefile
index b6746679c..2bc0ed5d4 100644
--- a/Makefile
+++ b/Makefile
@@ -5,4 +5,4 @@
-EXTRAVERSION =-apedf1
+EXTRAVERSION =-apedf1-trace
 NAME = Kleptomaniac Octopus
 
 # *DOCUMENTATION*
diff --git a/kernel/sched/cpufreq_schedutil.c b/kernel/sched/cpufreq_schedutil.c
index 831fee509..64b9f16bc 100644
--- a/kernel/sched/cpufreq_schedutil.c
+++ b/kernel/sched/cpufreq_schedutil.c
@@ -173,6 +173,10 @@ static unsigned int get_next_freq(struct sugov_policy *sg_policy,
 	unsigned int freq = arch_scale_freq_invariant() ?
 				policy->cpuinfo.max_freq : policy->cur;
 
+	/* Only for our tests for SCHED_DEADLINE in a controlled environment! */
+	if (freq > 1400000)
+		freq = 1400000;
+
 	freq = map_util_freq(util, freq, max);
 
 	if (freq == sg_policy->cached_raw_freq && !sg_policy->need_freq_update)
@@ -210,10 +214,25 @@ unsigned long schedutil_cpu_util(int cpu, unsigned long util_cfs,
 	unsigned long dl_util, util, irq;
 	struct rq *rq = cpu_rq(cpu);
 
+	/*
+	 * Throughout this function I commented some code
+	 * that might pollute our tests on the
+	 * energy-savings of our patches to DEADLINE.
+	 * During our tests, we don't give a damn about
+	 * lower-priority tasks, so long as the kernel does
+	 * not panic we are good.
+	 *
+	 * If you see some code commented out, remember that
+	 * it should NOT be commented out in any real
+	 * environment outside those tests.
+	 */
+
+	/*
 	if (!uclamp_is_used() &&
 	    type == FREQUENCY_UTIL && rt_rq_is_runnable(&rq->rt)) {
 		return max;
 	}
+	*/
 
 	/*
 	 * Early check to see if IRQ/steal time saturates the CPU, can be
@@ -236,9 +255,11 @@ unsigned long schedutil_cpu_util(int cpu, unsigned long util_cfs,
 	 * When there are no CFS RUNNABLE tasks, clamps are released and
 	 * frequency will be gracefully reduced with the utilization decay.
 	 */
+	/*
 	util = util_cfs + cpu_util_rt(rq);
 	if (type == FREQUENCY_UTIL)
 		util = uclamp_util_with(rq, util, p);
+	*/
 
 	dl_util = cpu_util_dl(rq);
 
@@ -537,6 +558,10 @@ sugov_update_shared(struct update_util_data *hook, u64 time, unsigned int flags)
 	if (sugov_should_update_freq(sg_policy, time)) {
 		next_f = sugov_next_freq_shared(sg_cpu, time);
 
+		/* Only for our tests in a controlled environment */
+		if (next_f > 1400000)
+			next_f = 1400000;
+
 		if (sg_policy->policy->fast_switch_enabled)
 			sugov_fast_switch(sg_policy, time, next_f);
 		else
diff --git a/kernel/sched/deadline.c b/kernel/sched/deadline.c
index 26c5c8200..332976397 100644
--- a/kernel/sched/deadline.c
+++ b/kernel/sched/deadline.c
@@ -1055,11 +1055,20 @@ static enum hrtimer_restart dl_task_timer(struct hrtimer *timer)
 		resched_curr(rq);
 
 #ifdef CONFIG_SMP
+
+	if (rq->dl.this_bw > rq->dl.max_bw) {
+		trace_printk(
+			"Rq %d bw overloaded\tthis_bw %d\tmax_bw %d\thas_pushable_dl_tasks %d\n",
+			rq, rq->dl.this_bw, rq->dl.max_bw, has_pushable_dl_tasks(rq));
+	}
+
 	/*
 	 * Queueing this task back might have overloaded rq, check if we need
 	 * to kick someone away.
 	 */
 	if (rq->dl.this_bw > rq->dl.max_bw && has_pushable_dl_tasks(rq)) {
+		trace_printk("RQ OVERLOAD TRIGGERED PUSH\n");
+
 		/*
 		 * Nothing relies on rq->lock after this, so its safe to drop
 		 * rq->lock.
@@ -1246,8 +1255,10 @@ static void update_curr_dl(struct rq *rq)
 			dl_se->dl_overrun = 1;
 
 		__dequeue_task_dl(rq, curr, 0);
-		if (unlikely(dl_se->dl_boosted || !start_dl_timer(curr)))
+		if (unlikely(dl_se->dl_boosted || !start_dl_timer(curr))) {
 			enqueue_task_dl(rq, curr, ENQUEUE_REPLENISH);
+			deadline_queue_push_tasks(rq);
+		}
 
 		if (!is_leftmost(curr, &rq->dl))
 			resched_curr(rq);
@@ -1801,9 +1812,9 @@ static void put_prev_task_dl(struct rq *rq, struct task_struct *p)
 	update_curr_dl(rq);
 
 	update_dl_rq_load_avg(rq_clock_pelt(rq), rq, 1);
-/* No! A preempted task cannot migrate!!!
+	// FIXME: No! A preempted task cannot migrate!!!
 	if (on_dl_rq(&p->dl) && p->nr_cpus_allowed > 1)
-		enqueue_pushable_dl_task(rq, p);*/
+		enqueue_pushable_dl_task(rq, p);
 }
 
 /*
@@ -2134,11 +2145,19 @@ static int push_dl_task(struct rq *rq)
 			return 0;
 		}
 
+		if (rq->dl.this_bw > rq->dl.max_bw) {
+			trace_printk("No later rq found using ff!\n");
+		}
+
 		/* Will lock the rq it'll find */
 		later_rq = find_lock_later_rq(next_task, rq);
 		if (!later_rq) {
 			struct task_struct *task;
 
+			if (rq->dl.this_bw > rq->dl.max_bw) {
+				trace_printk("No later rq found using the base rule!\n");
+			}
+
 			/*
 			 * We must check all this again, since
 			 * find_lock_later_rq releases rq->lock and it is
@@ -2150,12 +2169,17 @@ static int push_dl_task(struct rq *rq)
 				 * The task is still there. We don't try
 				 * again, some other CPU will pull it when ready.
 				 */
+				if (rq->dl.this_bw > rq->dl.max_bw) {
+					trace_printk("The task is still there! Must be pulled later!\n");
+				}
 				goto out;
 			}
 
-			if (!task)
+			if (!task) {
 				/* No more tasks */
+				trace_printk("The task is no longer there! Problem solved!\n");
 				goto out;
+			}
 
 			put_task_struct(next_task);
 			next_task = task;
@@ -2163,6 +2187,10 @@ static int push_dl_task(struct rq *rq)
 		}
 	}
 
+	if (rq->dl.this_bw > rq->dl.max_bw) {
+		trace_printk("Found a rq to push the task away!\n");
+	}
+
 	deactivate_task(rq, next_task, 0);
 	set_task_cpu(next_task, later_rq->cpu);
 
@@ -2279,6 +2307,7 @@ static void pull_dl_task(struct rq *this_rq)
 static void task_woken_dl(struct rq *rq, struct task_struct *p)
 {
 	if (rq->dl.this_bw > rq->dl.max_bw) {
+		trace_printk("TASK WAKE TRIGGERED PUSHES\n");
 		push_dl_tasks(rq);
 	}
 }
-- 
2.38.1

